---
title: "p8105_hw6_sx2402"
author: "Eric Xu"
date: "2025-11-28"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(p8105.datasets)
library(broom)
library(purrr)
library(modelr)
```

## Problem 1

### 1. Import and prepare data

```{r P1.1, message = FALSE}
homicide_raw = read_csv(
  "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
)

homicide_df = homicide_raw |>
  janitor::clean_names() |>
  mutate(
    city_state = str_c(city, ", ", state),
    resolved = if_else(disposition == "Closed by arrest", 1, 0),
    resolved = as.numeric(resolved),
    victim_age = case_when(victim_age == "Unknown" ~ NA, .default = victim_age),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")
  ) |>
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  ) |>
  mutate(
    victim_sex = fct_relevel(victim_sex, "Female"),
    victim_race = fct_relevel(victim_race, "White")
  )
```

### 2. Fit logistic regression on Baltimore data.

```{r P1.2}
baltimore_df = homicide_df |>
  filter(city_state == "Baltimore, MD")

baltimore_fit = 
  baltimore_df |> 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = _, family = binomial()) 



baltimore_results = baltimore_fit |>
  tidy(conf.int = TRUE, exponentiate = TRUE) |>
  filter(term == "victim_sexMale") |> mutate(adjusted_OR = estimate)|> 
  select(adjusted_OR, conf.low, conf.high)

kable(baltimore_results)
```

### 3. Fit for city in one pipeline

```{r P1.3, warning = FALSE}
city_results = homicide_df |>
  group_by(city_state) |>
  nest() |>
  mutate(
    model = map(
      data, 
      ~ glm(
          resolved ~ victim_age + victim_race + victim_sex,
          data = .x,
          family = binomial()
        )
    )
    ) |> 
  mutate(
    tidy = map(
      model,
      ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)
    )
  ) |> 
  unnest(tidy) |> 
  filter(term == "victim_sexMale") |>
    mutate(
    adjusted_OR = estimate
  ) |> 
  select(
    city_state,
    adjusted_OR,
    conf.low, 
    conf.high
  )
kable(city_results)
```

### 4. Plotting the adjusted odds ratios.

```{r P1.4, fig_height = 12, fig.width=7}
city_plot = city_results |> 
  ggplot(aes(x = adjusted_OR, y = fct_reorder(city_state, adjusted_OR)))+
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  labs(
    title = "OR of resolved case: Male vs Female Victim",
    x = "Adjusted OR",
    y = NULL
  ) +
  theme_minimal()+
  theme(
    axis.text.y = element_text(size = 6)
  )
city_plot
```

Comment:
A half of the cities have a wide confidence interval including the null value. This suggests not enough evidence of male victims having higher odds of resolved cases compared to female after adjusting for race and age. Other cities showing a significance usually have a odds ratio smaller than 1. This shows that in these cities, it is statistically significant that male victims have lower odds of resolved cases compared to female after adjusting for race and age. The lower OR estimates are more stable with lower SE and narrow CI.

## Problem 2

### 1. Import and prepare data

```{r P2, cache = TRUE}
data("weather_df")

boot_df = weather_df |>
  drop_na(tmax, tmin, prcp) |> 
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, tidy),
    glance = map(models, glance)) |> 
  select(-strap, -models) |> 
  unnest(results, names_sep = "_") |>
  unnest(glance, names_sep = "_")

boot_results = boot_df |> group_by(.id,results_term) |> 
  mutate(
    r2 = glance_r.squared
  ) |> 
  select(results_term,results_estimate,r2,.id) |> 
    pivot_wider(
    names_from = results_term,
    values_from = results_estimate
  ) |>
  mutate(
    beta_ratio = tmin / prcp
  )

boot_plot = boot_results |>
  select(r2, beta_ratio,.id) |>
  pivot_longer(
    cols = c(r2, beta_ratio),
    names_to = "estimate",
    values_to = "value"
  )
boot_plot |> 
  ggplot(aes(x = value)) +
  geom_density(fill = "steelblue", alpha = 0.5) +
  facet_wrap(vars(estimate),scales = "free") +
  theme_minimal() +
  labs(
    title = "Bootstrap Distributions",
    x = "Estimate",
    y = "Density"
  )

```

Both r squared and beta quotient in plot are slightly left skewed distribution. While it is unimodal and approximately symmetric, this suggests that the bootstrap procedure is stable and that the statistics behave regularly under repeated sampling.

### 2. Bootstraping summary

```{r P2.1, cache = TRUE}
boot_ci = boot_plot |>
  group_by(estimate) |> 
  summarize(
    ci_lower = quantile(value, 0.025),
    ci_upper = quantile(value, 0.975)
  )
kable(boot_ci, digits = 3)
```


## Problem 3

### 1. Import and prepare data

```{r P3.1}
birthweight_data = read.csv("birthweight.csv") |> 
  janitor::clean_names() |>
  mutate(
    babysex = factor(babysex, labels = c("Male", "Female")),
    frace = factor(frace,
                   levels = c(1,2,3,4,8,9),
                   labels = c("White","Black","Asian","Puerto Rican","Other","Unknown")),
    mrace = factor(mrace,
                   levels = c(1,2,3,4,8),
                   labels = c("White","Black","Asian","Puerto Rican","Other")),
    malform = factor(malform, labels = c("Absent","Present"))
    ) |> drop_na()
```


### 2. Model 1

```{r P3.2}
model1 = birthweight_data |> lm(
bwt ~ bhead + blength + gaweeks + babysex,data = _)

summary(model1)

birthweight_resid = birthweight_data |>
  add_predictions(model1) |>
  add_residuals(model1)

birthweight_resid |>
ggplot(aes(x = pred, y = resid)) +
geom_point(alpha = 0.4) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
theme_minimal() +
labs(
title = "Residuals vs Fitted Values for Model 1",
x = "Fitted Values",
y = "Residuals"
)
```

This model use a data-driven model-building process that models the birth weight from the fetal growth perspective that including the fetal status only.
The residuals are centered around zero with slight curvature with low fitted value. 
Variability has no obvious change at higher fitted values, and seemingly no major violations of linearity.

### 3. Cross validation

```{r P3.3}

cv_df =
  crossv_mc(birthweight_data, 100) |>
  mutate(
    train = map(train, as_tibble),
    test  = map(test,  as_tibble),
    fit_1 = map(train, ~ lm(bwt ~ bhead + blength + gaweeks + babysex, data = .x)),
    fit_2 = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    fit_3 = map(train, ~ lm(bwt ~ bhead * blength * babysex, data = .x)),
    rmse_1 = map2_dbl(fit_1, test, ~ rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(fit_2, test, ~ rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(fit_3, test, ~ rmse(model = .x, data = .y))
)


cv_results = cv_df |>
  select(starts_with("rmse")) |>
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse")

cv_summary = cv_results |>
  group_by(model) |>
  summarize(
    mean_rmse = mean(rmse),
    sd_rmse   = sd(rmse)
    )

cv_results |> ggplot(aes(x = model, y = rmse)) + geom_violin()


kable(cv_summary, digits = 3)
```

Model 1 have a relatively low RMSE distribution, compared to other model. However, model 1 does not account for interaction terms and has more estimators compared to model 2. Model 3 is more complex with full interaction terms but does not improve prediction and is seemingly over-fit. Overall, model 1 would be the better pick.
